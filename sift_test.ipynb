{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT / BoW / MHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "matcher = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "sift = cv2.SIFT_create(nfeatures=100)\n",
    "\n",
    "cluster_size = 20\n",
    "tc = (cv2.TERM_CRITERIA_MAX_ITER, 10, 0.001)\n",
    "retries = 3\n",
    "flags = cv2.KMEANS_PP_CENTERS;\n",
    "\n",
    "bow_trainer = cv2.BOWKMeansTrainer(cluster_size, tc, retries, flags)\n",
    "bow_descriptor_extractor = cv2.BOWImgDescriptorExtractor(sift, matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "jab_depth_files = [file for file in glob.glob('./numpy_data_arrays/jab/*depth.npz')]\n",
    "cross_depth_files = [file for file in glob.glob('./numpy_data_arrays/cross/*depth.npz')]\n",
    "left_hook_depth_files = [file for file in glob.glob('./numpy_data_arrays/left_hook/*depth.npz')]\n",
    "right_hook_depth_files = [file for file in glob.glob('./numpy_data_arrays/right_hook/*depth.npz')]\n",
    "random_depth_files = [file for file in glob.glob('./numpy_data_arrays/random/*depth.npz')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "THRESHOLD_VALUE = 5\n",
    "MHI_DURATION = 2\n",
    "\n",
    "def generate_mhi(frames):\n",
    "    number_of_frames = frames.shape[0]\n",
    "    height = frames.shape[1]\n",
    "    width = frames.shape[2]\n",
    "    SAMHI_10 = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "    for i in range(1, number_of_frames):\n",
    "        frame = frames[i]\n",
    "        frame[frame > 2200] = 0\n",
    "\n",
    "        image_binary = frame.astype(np.uint8)\n",
    "\n",
    "        num = 2\n",
    "        image_binary_prev = np.zeros((height, width), dtype=np.uint8)\n",
    "        difference = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "        if i == 1:\n",
    "            image_binary_prev = image_binary\n",
    "        elif (i % num) == 0:\n",
    "            difference = cv2.absdiff(image_binary_prev, image_binary)\n",
    "            image_binary_prev = image_binary\n",
    "\n",
    "        if i == num + 1:\n",
    "            _, image_binary_diff_5 = cv2.threshold(difference, THRESHOLD_VALUE, 255, cv2.THRESH_BINARY)\n",
    "            SAMHI_10 = cv2.motempl.updateMotionHistory(image_binary_diff_5, SAMHI_10, i / number_of_frames, MHI_DURATION)\n",
    "\n",
    "        if (i > num + 1 and i % num == 0):\n",
    "            _, image_binary_diff_5 = cv2.threshold(difference, THRESHOLD_VALUE, 255, cv2.THRESH_BINARY)\n",
    "            SAMHI_10 = cv2.motempl.updateMotionHistory(image_binary_diff_5, SAMHI_10, i / number_of_frames, MHI_DURATION)\n",
    "\n",
    "\n",
    "    SAMHI_10 = cv2.convertScaleAbs(SAMHI_10, alpha=255, beta=0)\n",
    "    return SAMHI_10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_image = None\n",
    "all_descriptors = []\n",
    "\n",
    "for file in jab_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(0, 255, 0), 0)\n",
    "\n",
    "for file in cross_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "for file in left_hook_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "for file in right_hook_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "# for file in random_depth_files:\n",
    "#     file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "#     depth_data = np.load(file)\n",
    "#     depth_frames = depth_data['arr_0']\n",
    "#     samhi = generate_mhi(depth_frames)\n",
    "#     keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "#     if descriptors is not None and descriptors.shape[0] > 0:\n",
    "#         all_descriptors.append(descriptors)\n",
    "#         bow_trainer.add(descriptors)\n",
    "#     keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "    # cv2.imshow(\"MHI\", samhi)\n",
    "    # cv2.imshow(\"Keypoint\", keyP1)\n",
    "    # if descriptors is not None and descriptors.shape[0] > 0:\n",
    "    #     cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "    # bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "\n",
    "    # dataset_descriptors.append(bow_descriptor)\n",
    "    # dataset_labels.append(1)\n",
    "\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "dictionary = bow_trainer.cluster()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_descriptor_extractor.setVocabulary(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_descriptors = []\n",
    "dataset_labels = []\n",
    "\n",
    "for file in jab_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "    # cv2.imshow(\"MHI\", samhi)\n",
    "    # cv2.imshow(\"Keypoint\", keyP1)\n",
    "    # if descriptors is not None and descriptors.shape[0] > 0:\n",
    "    #     cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "    bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "    if bow_descriptor is not None and descriptors.shape[0] > 0:\n",
    "        dataset_descriptors.append(bow_descriptor)\n",
    "        dataset_labels.append(\"jab\")\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "\n",
    "for file in cross_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "    # cv2.imshow(\"MHI\", samhi)\n",
    "    # cv2.imshow(\"Keypoint\", keyP1)\n",
    "    # if descriptors is not None and descriptors.shape[0] > 0:\n",
    "    #     cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "    bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "    if bow_descriptor is not None and descriptors.shape[0] > 0:\n",
    "        dataset_descriptors.append(bow_descriptor)\n",
    "        dataset_labels.append(\"cross\")\n",
    "\n",
    "for file in left_hook_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "    # cv2.imshow(\"MHI\", samhi)\n",
    "    # cv2.imshow(\"Keypoint\", keyP1)\n",
    "    # if descriptors is not None and descriptors.shape[0] > 0:\n",
    "    #     cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "    bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "    if bow_descriptor is not None and descriptors.shape[0] > 0:\n",
    "        dataset_descriptors.append(bow_descriptor)\n",
    "        dataset_labels.append(\"left_hook\")\n",
    "\n",
    "for file in right_hook_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "    # cv2.imshow(\"MHI\", samhi)\n",
    "    # cv2.imshow(\"Keypoint\", keyP1)\n",
    "    # if descriptors is not None and descriptors.shape[0] > 0:\n",
    "    #     cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "    bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "    if bow_descriptor is not None and descriptors.shape[0] > 0:\n",
    "        dataset_descriptors.append(bow_descriptor)\n",
    "        dataset_labels.append(\"right_hook\")\n",
    "\n",
    "# for file in random_depth_files:\n",
    "#     file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "#     depth_data = np.load(file)\n",
    "#     depth_frames = depth_data['arr_0']\n",
    "#     samhi = generate_mhi(depth_frames)\n",
    "#     keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "#     if descriptors is not None and descriptors.shape[0] > 0:\n",
    "#         all_descriptors.append(descriptors)\n",
    "#         bow_trainer.add(descriptors)\n",
    "#     keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "#     # cv2.imshow(\"MHI\", samhi)\n",
    "#     # cv2.imshow(\"Keypoint\", keyP1)\n",
    "#     # if descriptors is not None and descriptors.shape[0] > 0:\n",
    "#     #     cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "#     bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "#     if bow_descriptor is not None and descriptors.shape[0] > 0:\n",
    "#         dataset_descriptors.append(bow_descriptor)\n",
    "#         dataset_labels.append(\"random\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "flattened_descriptors = []\n",
    "for dataset_descriptor in dataset_descriptors:\n",
    "    flattened_descriptors.append(dataset_descriptor.flatten())\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(dataset_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(flattened_descriptors, encoded_labels, test_size=0.3)\n",
    "\n",
    "classes = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "shuffled_X, shuffled_y = shuffle(np.array(flattened_descriptors), np.array(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "def run_kfolds(model, x, y, splits):\n",
    "    kf = KFold(n_splits=splits, shuffle=True)\n",
    "    scores = cross_val_score(model, x, y, cv=kf, scoring=\"accuracy\")\n",
    "    predict = cross_val_predict(model, x, y, cv=kf)\n",
    "    return scores, predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SVM = SVC(kernel='rbf', gamma=0.50625000000000009, C=312.5)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "n_samples = X_test.shape[0]\n",
    "X_test = X_test.reshape(n_samples, -1)\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = SVM.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "svm_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"SVM Classification Report:\")\n",
    "print(svm_report)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Get the class labels from LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "SVM = SVC(kernel='rbf', gamma=0.50625000000000009, C=312.5)\n",
    "svm_scores, svm_predicts = run_kfolds(SVM, shuffled_X, shuffled_y, 5)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(shuffled_y, svm_predicts)\n",
    "svm_report = classification_report(shuffled_y, svm_predicts)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"SVM Cross Validation Classification Report:\")\n",
    "print(svm_report)\n",
    "\n",
    "cm = confusion_matrix(shuffled_y, svm_predicts)\n",
    "# Get the class labels from LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB = GaussianNB()\n",
    "\n",
    "NB.fit(X_train, y_train)\n",
    "y_pred_nb = NB.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "svm_report = classification_report(y_test, y_pred_nb)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Naive Bayes Classification Report:\")\n",
    "print(svm_report)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "# Get the class labels from LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(3,3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB1 = GaussianNB()\n",
    "\n",
    "scores, predicts = run_kfolds(NB1, shuffled_X, shuffled_y, 5)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(shuffled_y, predicts)\n",
    "svm_report = classification_report(shuffled_y, predicts)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Naive-Bayes Cross-Validation Classification Report:\")\n",
    "print(svm_report)\n",
    "\n",
    "cm = confusion_matrix(shuffled_y, predicts)\n",
    "# Get the class labels from LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(3, 3))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "scores, predicts = run_kfolds(rf, shuffled_X, shuffled_y, 5)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(shuffled_y, predicts)\n",
    "rf_report = classification_report(shuffled_y, predicts)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Random Forest Cross-Validation Classification Report:\")\n",
    "print(rf_report)\n",
    "\n",
    "cm = confusion_matrix(shuffled_y, predicts)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(3, 3))\n",
    "class_labels = label_encoder.classes_\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues' , xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# initialize the classifier\n",
    "#MLP = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "MLP = MLPClassifier(max_iter=10000)\n",
    "scores, predicts = run_kfolds(MLP, shuffled_X, shuffled_y, 5)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(shuffled_y, predicts)\n",
    "mlp_report = classification_report(shuffled_y, predicts)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"MLP Cross-Validation Classification Report:\")\n",
    "print(mlp_report)\n",
    "\n",
    "cm = confusion_matrix(shuffled_y, predicts)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(3, 3))\n",
    "class_labels = label_encoder.classes_\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues' , xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
