{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "matcher = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "sift = cv2.SIFT_create(nfeatures=200)\n",
    "\n",
    "cluster_size = 10\n",
    "tc = (cv2.TERM_CRITERIA_MAX_ITER, 10, 0.001)\n",
    "retries = 3\n",
    "flags = cv2.KMEANS_PP_CENTERS;\n",
    "\n",
    "bow_trainer = cv2.BOWKMeansTrainer(cluster_size, tc, retries, flags)\n",
    "bow_descriptor_extractor = cv2.BOWImgDescriptorExtractor(sift, matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "jab_depth_files = [file for file in glob.glob('./numpy_data_arrays/jab/*depth.npz')]\n",
    "cross_depth_files = [file for file in glob.glob('./numpy_data_arrays/cross/*depth.npz')]\n",
    "left_hook_depth_files = [file for file in glob.glob('./numpy_data_arrays/left_hook/*depth.npz')]\n",
    "right_hook_depth_files = [file for file in glob.glob('./numpy_data_arrays/right_hook/*depth.npz')]\n",
    "random_depth_files = [file for file in glob.glob('./numpy_data_arrays/random/*depth.npz')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "THRESHOLD_VALUE = 5\n",
    "MHI_DURATION = 2\n",
    "\n",
    "def generate_mhi(frames):\n",
    "    number_of_frames = frames.shape[0]\n",
    "    height = frames.shape[1]\n",
    "    width = frames.shape[2]\n",
    "    SAMHI_10 = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "    for i in range(1, number_of_frames):\n",
    "        frame = frames[i]\n",
    "        frame[frame > 2200] = 0\n",
    "\n",
    "        image_binary = frame.astype(np.uint8)\n",
    "\n",
    "        num = 5\n",
    "        image_binary_prev = np.zeros((height, width), dtype=np.uint8)\n",
    "        difference = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "        if i == 1:\n",
    "            image_binary_prev = image_binary\n",
    "        elif (i % num) == 0:\n",
    "            difference = cv2.absdiff(image_binary_prev, image_binary)\n",
    "            image_binary_prev = image_binary\n",
    "\n",
    "        if i == num + 1:\n",
    "            _, image_binary_diff_5 = cv2.threshold(difference, THRESHOLD_VALUE, 255, cv2.THRESH_BINARY)\n",
    "            SAMHI_10 = cv2.motempl.updateMotionHistory(image_binary_diff_5, SAMHI_10, i / number_of_frames, MHI_DURATION)\n",
    "\n",
    "        if (i > num + 1 and i % num == 0):\n",
    "            _, image_binary_diff_5 = cv2.threshold(difference, THRESHOLD_VALUE, 255, cv2.THRESH_BINARY)\n",
    "            SAMHI_10 = cv2.motempl.updateMotionHistory(image_binary_diff_5, SAMHI_10, i / number_of_frames, MHI_DURATION)\n",
    "\n",
    "\n",
    "    SAMHI_10 = cv2.convertScaleAbs(SAMHI_10, alpha=255, beta=0)\n",
    "    return SAMHI_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_image = None\n",
    "all_descriptors = []\n",
    "\n",
    "for file in jab_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "for file in cross_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "for file in left_hook_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "for file in right_hook_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "# for file in random_depth_files[0:5]:\n",
    "#     file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "#     depth_data = np.load(file)\n",
    "#     depth_frames = depth_data['arr_0']\n",
    "#     samhi = generate_mhi(depth_frames)\n",
    "#     keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "#     if descriptors is not None and descriptors.shape[0] > 0:\n",
    "#         all_descriptors.append(descriptors)\n",
    "#         bow_trainer.add(descriptors)\n",
    "#     keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "    # cv2.imshow(\"MHI\", samhi)\n",
    "    # cv2.imshow(\"Keypoint\", keyP1)\n",
    "    # if descriptors is not None and descriptors.shape[0] > 0:\n",
    "    #     cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "    # bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "\n",
    "    # dataset_descriptors.append(bow_descriptor)\n",
    "    # dataset_labels.append(1)\n",
    "\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "dictionary = bow_trainer.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_descriptor_extractor.setVocabulary(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_descriptors = []\n",
    "dataset_labels = []\n",
    "\n",
    "for file in jab_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "    cv2.imshow(\"MHI\", samhi)\n",
    "    cv2.imshow(\"Keypoint\", keyP1)\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "    bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "    if bow_descriptor is not None and descriptors.shape[0] > 0:\n",
    "        dataset_descriptors.append(bow_descriptor)\n",
    "        dataset_labels.append(\"jab\")\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "for file in cross_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "    # cv2.imshow(\"MHI\", samhi)\n",
    "    # cv2.imshow(\"Keypoint\", keyP1)\n",
    "    # if descriptors is not None and descriptors.shape[0] > 0:\n",
    "    #     cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "    bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "    if bow_descriptor is not None and descriptors.shape[0] > 0:\n",
    "        dataset_descriptors.append(bow_descriptor)\n",
    "        dataset_labels.append(\"cross\")\n",
    "\n",
    "for file in left_hook_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "    # cv2.imshow(\"MHI\", samhi)\n",
    "    # cv2.imshow(\"Keypoint\", keyP1)\n",
    "    # if descriptors is not None and descriptors.shape[0] > 0:\n",
    "    #     cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "    bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "    if bow_descriptor is not None and descriptors.shape[0] > 0:\n",
    "        dataset_descriptors.append(bow_descriptor)\n",
    "        dataset_labels.append(\"left_hook\")\n",
    "\n",
    "for file in right_hook_depth_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    depth_data = np.load(file)\n",
    "    depth_frames = depth_data['arr_0']\n",
    "    samhi = generate_mhi(depth_frames)\n",
    "    keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "    if descriptors is not None and descriptors.shape[0] > 0:\n",
    "        all_descriptors.append(descriptors)\n",
    "        bow_trainer.add(descriptors)\n",
    "    keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "    # cv2.imshow(\"MHI\", samhi)\n",
    "    # cv2.imshow(\"Keypoint\", keyP1)\n",
    "    # if descriptors is not None and descriptors.shape[0] > 0:\n",
    "    #     cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "    bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "    if bow_descriptor is not None and descriptors.shape[0] > 0:\n",
    "        dataset_descriptors.append(bow_descriptor)\n",
    "        dataset_labels.append(\"right_hook\")\n",
    "\n",
    "# for file in random_depth_files[0:5]:\n",
    "#     file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "#     depth_data = np.load(file)\n",
    "#     depth_frames = depth_data['arr_0']\n",
    "#     samhi = generate_mhi(depth_frames)\n",
    "#     keypoint, descriptors = sift.detectAndCompute(samhi, None)\n",
    "\n",
    "#     if descriptors is not None and descriptors.shape[0] > 0:\n",
    "#         all_descriptors.append(descriptors)\n",
    "#         bow_trainer.add(descriptors)\n",
    "#     keyP1 = cv2.drawKeypoints(samhi,keypoint, None,(255, 255, 255), 0)\n",
    "\n",
    "#     # cv2.imshow(\"MHI\", samhi)\n",
    "#     # cv2.imshow(\"Keypoint\", keyP1)\n",
    "#     # if descriptors is not None and descriptors.shape[0] > 0:\n",
    "#     #     cv2.imshow(\"Features\", descriptors)\n",
    "\n",
    "#     bow_descriptor = bow_descriptor_extractor.compute(samhi, keypoint)\n",
    "#     if bow_descriptor is not None and descriptors.shape[0] > 0:\n",
    "#         dataset_descriptors.append(bow_descriptor)\n",
    "#         dataset_labels.append(\"random\")\n",
    "#     # cv2.waitKey(0)\n",
    "#     # cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "flattened_descriptors = []\n",
    "for dataset_descriptor in dataset_descriptors:\n",
    "    flattened_descriptors.append(dataset_descriptor.flatten())\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(dataset_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(flattened_descriptors, encoded_labels, test_size=0.3)\n",
    "\n",
    "\n",
    "classes = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_descriptors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/50835014\n",
    "\n",
    "def get_integer_mapping(le):\n",
    "    '''\n",
    "    Return a dict mapping labels to their integer values\n",
    "    from an SKlearn LabelEncoder\n",
    "    le = a fitted SKlearn LabelEncoder\n",
    "    '''\n",
    "    res = {}\n",
    "    for cl in le.classes_:\n",
    "        res.update({cl:le.transform([cl])[0]})\n",
    "\n",
    "    return res\n",
    "\n",
    "integerMapping = get_integer_mapping(label_encoder)\n",
    "for item in ['jab', 'cross', 'left_hook', 'right_hook']:\n",
    "    print(integerMapping[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "SVM = SVC(kernel='rbf', gamma=0.50625000000000009, C=312.5)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "SVM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "n_samples = X_test.shape[0]\n",
    "X_test = X_test.reshape(n_samples, -1)\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = SVM.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "svm_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"SVM Classification Report:\")\n",
    "print(svm_report)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Get the class labels from LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB = GaussianNB()\n",
    "\n",
    "NB.fit(X_train, y_train)\n",
    "y_pred_nb = NB.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "svm_report = classification_report(y_test, y_pred_nb)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Naive Bayes Classification Report:\")\n",
    "print(svm_report)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_nb)\n",
    "# Get the class labels from LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GaussianNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_val_score, cross_val_predict\n\u001b[1;32m----> 3\u001b[0m NB1 \u001b[39m=\u001b[39m GaussianNB()\n\u001b[0;32m      4\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(flattened_descriptors)\n\u001b[0;32m      5\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(encoded_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GaussianNB' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "NB1 = GaussianNB()\n",
    "X = np.array(flattened_descriptors)\n",
    "y = np.array(encoded_labels)\n",
    "scores = cross_val_score(NB1, X, y, cv=10)\n",
    "\n",
    "# print(f'Accuracy {scores.mean()} (+/- {scores.std() * 2})')\n",
    "\n",
    "y_pred_nb_cv = cross_val_predict(NB1, X, y, cv=10)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y, y_pred_nb_cv)\n",
    "svm_report = classification_report(y, y_pred_nb_cv)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Naive-Bayes Cross-Validation Classification Report:\")\n",
    "print(svm_report)\n",
    "\n",
    "cm = confusion_matrix(y, y_pred_nb_cv)\n",
    "# Get the class labels from LabelEncoder\n",
    "class_labels = label_encoder.classes_\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
