{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, LSTM, Dropout, BatchNormalization, Dense, Conv1D, MaxPooling1D, Flatten, Concatenate\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jab_depth_files = [file for file in glob.glob('./numpy_data_arrays/jab/*depth.npz')]\n",
    "# cross_depth_files = [file for file in glob.glob('./numpy_data_arrays/cross/*depth.npz')]\n",
    "# left_hook_depth_files = [file for file in glob.glob('./numpy_data_arrays/left_hook/*depth.npz')]\n",
    "# right_hook_depth_files = [file for file in glob.glob('./numpy_data_arrays/right_hook/*depth.npz')]\n",
    "# random_depth_files = [file for file in glob.glob('./numpy_data_arrays/random/*depth.npz')]\n",
    "\n",
    "# jab_color_files = [file for file in glob.glob('./numpy_data_arrays/jab/*color.npz')]\n",
    "# cross_color_files = [file for file in glob.glob('./numpy_data_arrays/cross/*color.npz')]\n",
    "# left_hook_color_files = [file for file in glob.glob('./numpy_data_arrays/left_hook/*color.npz')]\n",
    "# right_hook_color_files = [file for file in glob.glob('./numpy_data_arrays/right_hook/*color.npz')]\n",
    "# random_color_files = [file for file in glob.glob('./numpy_data_arrays/random/*color.npz')]\n",
    "\n",
    "data_files = [file for file in glob.glob('./numpy_data_arrays/*/*.npz')]\n",
    "\n",
    "row_names = []\n",
    "\n",
    "for file in data_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    row_name = file_name[:-6]\n",
    "    if not row_name in row_names: \n",
    "        row_names.append(row_name)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest_num_of_frames = float('inf')\n",
    "# for row_name in row_names:\n",
    "#     number_of_frames = data_dict[row_name]['number_of_frames']\n",
    "#     if number_of_frames < lowest_num_of_frames:\n",
    "#         lowest_num_of_frames = number_of_frames\n",
    "\n",
    "# lowest_num_of_frames = 6\n",
    "\n",
    "# Extract 6 frames evenly spread out\n",
    "def extract_frames(frames):\n",
    "    extracted_frames = []\n",
    "    for i in range(1, 7):\n",
    "        frame_to_extract = (i * (frames.shape[0]) // 6 ) - 1\n",
    "        extracted_frames.append(frames[frame_to_extract])\n",
    "    return np.array(extracted_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 383, 383, 4)\n",
      "(21, 383, 383)\n",
      "(16, 383, 383, 4)\n",
      "(16, 383, 383)\n",
      "(28, 383, 383)\n",
      "(28, 383, 383, 4)\n",
      "(29, 383, 383)\n",
      "(29, 383, 383, 4)\n",
      "(21, 383, 383, 4)\n",
      "(21, 383, 383)\n",
      "(15, 383, 383)\n",
      "(15, 383, 383, 4)\n",
      "(18, 383, 383, 4)\n",
      "(18, 383, 383)\n",
      "(28, 383, 383)\n",
      "(28, 383, 383, 4)\n",
      "(30, 383, 383, 4)\n",
      "(30, 383, 383)\n",
      "(20, 383, 383, 4)\n",
      "(20, 383, 383)\n",
      "(38, 383, 383)\n",
      "(38, 383, 383, 4)\n",
      "(27, 383, 383, 4)\n",
      "(27, 383, 383)\n",
      "(30, 383, 383)\n",
      "(30, 383, 383, 4)\n",
      "(28, 383, 383)\n",
      "(28, 383, 383, 4)\n",
      "(30, 383, 383)\n",
      "(30, 383, 383, 4)\n",
      "(30, 383, 383)\n",
      "(30, 383, 383, 4)\n",
      "(19, 383, 383, 4)\n",
      "(19, 383, 383)\n",
      "(20, 383, 383)\n",
      "(20, 383, 383, 4)\n",
      "(6, 383, 383, 4)\n",
      "(6, 383, 383)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(10, 383, 383)\n",
      "(10, 383, 383, 4)\n",
      "(25, 383, 383, 4)\n",
      "(25, 383, 383)\n",
      "(17, 383, 383)\n",
      "(17, 383, 383, 4)\n",
      "(17, 383, 383, 4)\n",
      "(17, 383, 383)\n",
      "(21, 383, 383, 4)\n",
      "(21, 383, 383)\n",
      "(21, 383, 383)\n",
      "(21, 383, 383, 4)\n",
      "(26, 383, 383)\n",
      "(26, 383, 383, 4)\n",
      "(29, 383, 383)\n",
      "(29, 383, 383, 4)\n",
      "(24, 383, 383)\n",
      "(24, 383, 383, 4)\n",
      "(34, 383, 383)\n",
      "(34, 383, 383, 4)\n",
      "(37, 383, 383)\n",
      "(37, 383, 383, 4)\n",
      "(32, 383, 383)\n",
      "(32, 383, 383, 4)\n",
      "(30, 383, 383)\n",
      "(30, 383, 383, 4)\n",
      "(30, 383, 383)\n",
      "(30, 383, 383, 4)\n",
      "(23, 383, 383)\n",
      "(23, 383, 383, 4)\n",
      "(27, 383, 383)\n",
      "(27, 383, 383, 4)\n",
      "(29, 383, 383)\n",
      "(29, 383, 383, 4)\n",
      "(26, 383, 383)\n",
      "(26, 383, 383, 4)\n",
      "(24, 383, 383)\n",
      "(24, 383, 383, 4)\n",
      "(27, 383, 383)\n",
      "(27, 383, 383, 4)\n",
      "(23, 383, 383)\n",
      "(23, 383, 383, 4)\n",
      "(19, 383, 383)\n",
      "(19, 383, 383, 4)\n",
      "(29, 383, 383)\n",
      "(29, 383, 383, 4)\n",
      "(20, 383, 383)\n",
      "(20, 383, 383, 4)\n",
      "(19, 383, 383)\n",
      "(19, 383, 383, 4)\n",
      "(25, 383, 383)\n",
      "(25, 383, 383, 4)\n",
      "(34, 383, 383)\n",
      "(34, 383, 383, 4)\n",
      "(30, 383, 383)\n",
      "(30, 383, 383, 4)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(30, 383, 383)\n",
      "(30, 383, 383, 4)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(27, 383, 383, 4)\n",
      "(27, 383, 383)\n",
      "(29, 383, 383)\n",
      "(29, 383, 383, 4)\n",
      "(21, 383, 383)\n",
      "(21, 383, 383, 4)\n",
      "(24, 383, 383)\n",
      "(24, 383, 383, 4)\n",
      "(29, 383, 383)\n",
      "(29, 383, 383, 4)\n",
      "(30, 383, 383)\n",
      "(30, 383, 383, 4)\n",
      "(24, 383, 383)\n",
      "(24, 383, 383, 4)\n",
      "(32, 383, 383, 4)\n",
      "(32, 383, 383)\n",
      "(31, 383, 383)\n",
      "(31, 383, 383, 4)\n",
      "(26, 383, 383)\n",
      "(26, 383, 383, 4)\n",
      "(24, 383, 383)\n",
      "(24, 383, 383, 4)\n",
      "(24, 383, 383)\n",
      "(24, 383, 383, 4)\n",
      "(25, 383, 383, 4)\n",
      "(25, 383, 383)\n",
      "(24, 383, 383)\n",
      "(24, 383, 383, 4)\n",
      "(17, 383, 383)\n",
      "(17, 383, 383, 4)\n",
      "(28, 383, 383)\n",
      "(28, 383, 383, 4)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(35, 383, 383, 4)\n",
      "(35, 383, 383)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(31, 383, 383)\n",
      "(31, 383, 383, 4)\n",
      "(25, 383, 383)\n",
      "(25, 383, 383, 4)\n",
      "(19, 383, 383, 4)\n",
      "(19, 383, 383)\n",
      "(19, 383, 383)\n",
      "(19, 383, 383, 4)\n",
      "(23, 383, 383, 4)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(21, 383, 383)\n",
      "(21, 383, 383, 4)\n",
      "(27, 383, 383)\n",
      "(27, 383, 383, 4)\n",
      "(33, 383, 383)\n",
      "(33, 383, 383, 4)\n",
      "(28, 383, 383, 4)\n",
      "(28, 383, 383)\n",
      "(25, 383, 383)\n",
      "(25, 383, 383, 4)\n",
      "(28, 383, 383)\n",
      "(28, 383, 383, 4)\n",
      "(19, 383, 383)\n",
      "(19, 383, 383, 4)\n",
      "(20, 383, 383)\n",
      "(20, 383, 383, 4)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(16, 383, 383)\n",
      "(16, 383, 383, 4)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(16, 383, 383)\n",
      "(16, 383, 383, 4)\n",
      "(19, 383, 383)\n",
      "(19, 383, 383, 4)\n",
      "(23, 383, 383)\n",
      "(23, 383, 383, 4)\n",
      "(25, 383, 383, 4)\n",
      "(25, 383, 383)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(24, 383, 383)\n",
      "(24, 383, 383, 4)\n",
      "(31, 383, 383)\n",
      "(31, 383, 383, 4)\n",
      "(24, 383, 383)\n",
      "(24, 383, 383, 4)\n",
      "(23, 383, 383, 4)\n",
      "(23, 383, 383)\n",
      "(26, 383, 383, 4)\n",
      "(26, 383, 383)\n",
      "(26, 383, 383)\n",
      "(26, 383, 383, 4)\n",
      "(23, 383, 383)\n",
      "(21, 383, 383)\n",
      "(21, 383, 383, 4)\n",
      "(26, 383, 383)\n",
      "(26, 383, 383, 4)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(21, 383, 383)\n",
      "(21, 383, 383, 4)\n",
      "(20, 383, 383)\n",
      "(20, 383, 383, 4)\n",
      "(19, 383, 383)\n",
      "(19, 383, 383, 4)\n",
      "(27, 383, 383)\n",
      "(27, 383, 383, 4)\n",
      "(25, 383, 383)\n",
      "(25, 383, 383, 4)\n",
      "(25, 383, 383)\n",
      "(25, 383, 383, 4)\n",
      "(27, 383, 383)\n",
      "(27, 383, 383, 4)\n",
      "(22, 383, 383)\n",
      "(22, 383, 383, 4)\n",
      "(19, 383, 383)\n",
      "(19, 383, 383, 4)\n",
      "(25, 383, 383)\n",
      "(25, 383, 383, 4)\n",
      "(23, 383, 383)\n",
      "(23, 383, 383, 4)\n",
      "(31, 383, 383)\n",
      "(31, 383, 383, 4)\n",
      "(27, 383, 383)\n",
      "(27, 383, 383, 4)\n",
      "(24, 383, 383)\n",
      "(24, 383, 383, 4)\n",
      "(20, 383, 383)\n",
      "(20, 383, 383, 4)\n",
      "(22, 383, 383, 4)\n",
      "(22, 383, 383)\n",
      "(25, 383, 383)\n",
      "(25, 383, 383, 4)\n",
      "(20, 383, 383)\n",
      "(20, 383, 383, 4)\n",
      "(25, 383, 383)\n",
      "(25, 383, 383, 4)\n",
      "(24, 383, 383, 4)\n",
      "(24, 383, 383)\n",
      "(21, 383, 383)\n",
      "(21, 383, 383, 4)\n",
      "(27, 383, 383)\n",
      "(27, 383, 383, 4)\n"
     ]
    }
   ],
   "source": [
    "data_dict = {}\n",
    "\n",
    "for file in data_files:\n",
    "    file_name = file.split(\"\\\\\")[-1].split('.')[0]\n",
    "    row_name = file_name[:-6]\n",
    "    depth_or_color = file_name[-5:]\n",
    "\n",
    "    loaded_file = np.load(file)\n",
    "    data = loaded_file['arr_0']\n",
    "    print(data.shape)\n",
    "\n",
    "    if row_name not in data_dict:\n",
    "        data_dict[row_name] = {}\n",
    "\n",
    "    if depth_or_color == 'color':\n",
    "        data_dict[row_name]['full_color'] = data\n",
    "        data_dict[row_name]['number_of_frames'] = data.shape[0]\n",
    "        data_dict[row_name]['six_color_frames'] = extract_frames(data)\n",
    "        if \"jab\" in row_name:\n",
    "            data_dict[row_name][\"label\"] = \"jab\"\n",
    "        elif \"cross\" in row_name:\n",
    "            data_dict[row_name][\"label\"] = \"cross\"\n",
    "        elif \"left_hook\" in row_name:\n",
    "            data_dict[row_name][\"label\"] = \"left_hook\"\n",
    "        elif \"right_hook\" in row_name:\n",
    "            data_dict[row_name][\"label\"] = \"right_hook\"\n",
    "        elif \"random\" in row_name:\n",
    "            data_dict[row_name][\"label\"] = \"random\"\n",
    "    elif depth_or_color == 'depth':\n",
    "        data_dict[row_name]['full_depth'] = data\n",
    "        data_dict[row_name]['six_depth_frames'] = extract_frames(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3628\n"
     ]
    }
   ],
   "source": [
    "# get max depth value\n",
    "max_depth = float('-inf')\n",
    "for row_name in row_names:\n",
    "    six_depth_frames =  data_dict[row_name]['six_depth_frames']\n",
    "    if six_depth_frames.max() > max_depth:\n",
    "        max_depth = six_depth_frames.max()\n",
    "    \n",
    "print(max_depth)\n",
    "# max_depth\n",
    "# Max depth found = 3628\n",
    "# bgr = data_dict['jab_1']['six_color_frames'][0][0][0][:-1]\n",
    "# bgr2 = data_dict['jab_1']['six_color_frames'][0][0][0][:-1] / 255\n",
    "# d = data_dict['jab_1']['six_depth_frames'][0][0][0] / 3628\n",
    "\n",
    "# print(np.array([*bgr, d]))\n",
    "# print(np.array([*bgr2, d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_name in row_names:\n",
    "    normalized_bgr = data_dict[row_name]['six_color_frames'][:][:][:][:] / 255\n",
    "    normalized_d = data_dict[row_name]['six_depth_frames'][:][:][:] / 3628 #3628 found to be max\n",
    "    normalized_bgr[..., 3] = normalized_d\n",
    "    data_dict[row_name]['six_bgrd'] = np.array(normalized_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "jab_dataset = [data_dict[row_name][\"six_bgrd\"] for row_name in row_names if \"jab\" in row_name]\n",
    "cross_dataset = [data_dict[row_name][\"six_bgrd\"] for row_name in row_names if \"cross\" in row_name]\n",
    "left_hook_dataset = [data_dict[row_name][\"six_bgrd\"] for row_name in row_names if \"left_hook\" in row_name]\n",
    "right_hook_dataset = [data_dict[row_name][\"six_bgrd\"] for row_name in row_names if \"right_hook\" in row_name]\n",
    "random_dataset = [data_dict[row_name][\"six_bgrd\"] for row_name in row_names if \"random\" in row_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(jab_dataset)\n",
    "random.shuffle(cross_dataset)\n",
    "random.shuffle(left_hook_dataset)\n",
    "random.shuffle(right_hook_dataset)\n",
    "random.shuffle(random_dataset)\n",
    "\n",
    "X_train = jab_dataset[:19] + cross_dataset[:19] + left_hook_dataset[:19] + right_hook_dataset[:19] + random_dataset[:19]\n",
    "y_train = ['jab'] * 19 + ['cross'] * 19 + ['left_hook'] * 19 + ['right_hook'] * 19 + ['random'] * 19\n",
    "X_test = jab_dataset[19:] + cross_dataset[19:] + left_hook_dataset[19:] + right_hook_dataset[19:] + random_dataset[19:]\n",
    "y_test = ['jab'] * 6 + ['cross'] * 6 + ['left_hook'] * 6 + ['right_hook'] * 6 + ['random'] * 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Classes: [1 2 3 4 5]\n",
      "Encoded Training Labels: [0 4 0 1 2 0 4 4 2 3 3 3 0 4 1 1 3 2 4 0 2 3 4 1 0 1 2 4 3 3 1 0 3 1 2 2 2\n",
      " 3 2 3 1 4 4 4 4 2 3 0 4 3 0 0 4 0 0 3 2 1 1 1 1 2 1 4 0 3 1 2 2 4 3 1 2 3\n",
      " 3 4 1 1 0 3 0 1 2 0 2 2 2 0 0 4 4 1 0 4 3]\n",
      "Encoded Testing Labels: [0 0 0 0 0 0 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "classes = label_encoder.classes_\n",
    "print(\"Unique Classes:\", classes)\n",
    "print(\"Encoded Training Labels:\", y_train_encoded)\n",
    "print(\"Encoded Testing Labels:\", y_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv3D(32, (3, 3, 3), activation='relu', input_shape=(6, 383, 383, 4)),\n",
    "    tf.keras.layers.MaxPooling3D((1, 2, 2)),\n",
    "    tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D((1, 2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_encoded, epochs=10, batch_size=32)\n",
    "\n",
    "model.evaluate(X_test, y_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Check if GPU is available\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNum GPUs Available:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(tf\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mlist_physical_devices(\u001b[39m'\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_2():\n",
    "    time_at_start = int(time())\n",
    "    conv_layers = [1, 2]\n",
    "    conv_layer_sizes = [16, 32]\n",
    "    dense_layers = [1, 2]\n",
    "    dense_layer_sizes = [32, 64]\n",
    "\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 32\n",
    "\n",
    "    for dense_layer in dense_layers:\n",
    "        for dense_layer_size in dense_layer_sizes:\n",
    "                    for conv_layer in conv_layers:\n",
    "                        for conv_layer_size in conv_layer_sizes:\n",
    "                            name = f\"{conv_layer}C{conv_layer_size}-{dense_layer}D{dense_layer_size}-{int(time())}\"\n",
    "\n",
    "                            model = Sequential()\n",
    "                            model.add(Input(shape=(cnn_train_x.shape[1:])))\n",
    "\n",
    "                            for layer in range(conv_layer - 1):\n",
    "                                model.add(Conv1D(conv_layer_size, kernel_size=3, padding='valid', activation='relu'))\n",
    "                                model.add(MaxPooling1D(1))\n",
    "                    \n",
    "                            model.add(Conv1D(conv_layer_size, kernel_size=3, padding='valid', activation='relu'))\n",
    "                            model.add(MaxPooling1D(1))\n",
    "\n",
    "                            model.add(Flatten())\n",
    "\n",
    "                            for layer in range(dense_layer - 1):\n",
    "                                model.add(Dense(dense_layer_size, activation='relu'))\n",
    "                                model.add(Dropout(0.4))\n",
    "\n",
    "                            model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "                            opt = tf.keras.optimizers.Adam(learning_rate=0.001, decay=1e-6)\n",
    "\n",
    "                            model.compile(loss='sparse_categorical_crossentropy',\n",
    "                                            optimizer=opt,\n",
    "                                            metrics=['accuracy'])\n",
    "\n",
    "                            tensorboard = TensorBoard(log_dir=f'cnniteration2logs-{time_at_start}/{name}')\n",
    "\n",
    "                            checkpoint_filepath = f\"cnniteration2models-{time_at_start}/\" + name + \"-{epoch:02d}-{val_accuracy:.3f}.hd5\"\n",
    "                            checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "                            early_stopping = EarlyStopping(monitor='val_accuracy', baseline=0.5, patience=12)\n",
    "\n",
    "                            history = model.fit(\n",
    "                                cnn_train_x, train_y,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                epochs=EPOCHS,\n",
    "                                validation_data=(cnn_validation_x, validation_y),\n",
    "                                callbacks=[tensorboard, checkpoint, early_stopping]\n",
    "                            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
